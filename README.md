# Домашнее задание №1. On-policy алгоритмы семейства актор-критик

## Теоретическое задание

Теоретическое задание состоит из части на алгоритм TD($\lambda$) и части на методы аппроксимации стратегии.

**Задание 1.** Доказать, что если вычислять обновления весов на каждом шаге (без их применения), то суммарное обновление онлайнового и оффлайного варианта одинаково.

*NB: Под оффлайновым обновлением здесь подразумевается оффлайновый алгоритм $\lambda$-отдачи. Под онлайновым обновлением подразумевается алгоритм TD($\lambda$). Подсказки можно найти в заданиях в конце главы 12.1 книги Саттона и Барто.*

**Задание 2.** Доказать равнозначность оффлайного алгоритма $\lambda$-отдачи и истинно онлайнового алгоритма TD($\lambda$).

*См. соответствующую главу 12.5 книги Саттона и Барто и статью [van Seijen et al., 2016](https://www.jmlr.org/papers/volume17/15-599/15-599.pdf).*

**Задание 3.** Воспользуйтесь своими знаниями о клеточном мире и его динамике, чтобы найти точное символьное выражение для оптимальной вероятности выбора действия `right` в примере 13.1 книги Саттона и Барто.

## Практическое задание

Практическое задание посвящено теме on-policy алгоритмов семейства актор-критик. Тетрадка с заданием представлена [в репозитории](Homework_ActorCritic.ipynb) и в [Google Colab](https://drive.google.com/file/d/1b3nosflVBgv1Si8toRc5IQ1bw0wl-ltU/view?usp=sharing).

## Формат сдачи

Сдача осуществляется путем создания Pull Request'а. Убедитесь, что:

- в PR есть и на видном месте финальная версия вашего кода: ссылкой на колаб и/или коммитом тетради.
- в коммитах нет лишнего мусора, output в ячейках ноутбука не требует бесконечно скроллить.
- на видном месте и в удобной для просмотра форме сам отчет (pdf, markdown, отчет в wandb и тп)
- на видном месте ссылка на логи запусков в wandb.
- текст читаемый, если теоретическое задание сдается письменно от руки (т.е. сканы/фото, объединенные в pdf)
