{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3f8puFn2S3a"
   },
   "source": [
    "# Введение\n",
    "Пусть $\\pi = \\pi (s, a; w)$ - аппроксимация стратегии с параметрами $w$. Тогда градиент стратеции имеет вид (см. лекции **5. Градиент стратегии**, $d ^ {\\pi}$ - траектории, сэмплируемые стратегией $\\pi$):\n",
    "$$ \\nabla_w J(w) = \\mathop{\\mathbb{E}}_{s \\sim d ^ {\\pi}, \\, a \\sim \\pi} \\left[ Q ^ {\\pi} (s, a) \\nabla_w \\log{\\pi(a | s; w)}\\right]$$\n",
    "\n",
    "Наиболее распространённым подходом в on-policy алгоритмах семейства актор-критик является оценка градиента $J$ (или суррогатного функционала, напр. в PPO) по состояниям и действиям, сэмплированным из траектории, полученной выполнением текущей стратегии в среде (см. семинар Актор-критик).\n",
    "\n",
    "В данной работе предлагается реализовать альтернативный вариант обучения на основе градиента стратегии, применимый в средах с конечным дискретным множеством действий. Обозначим $\\mathcal{A} = \\{a \\}$ - дискретное множеством действий. Тогда мы можем переписать выражение следующим образом ($\\hat{Q}(s, a; u)$ - аппроксиматор $Q$ с параметрами $u$):\n",
    "$$\\begin{align}\n",
    "\\nabla_w J(w) &= \\mathop{\\mathbb{E}}_{s \\sim d ^ {\\pi}, \\, a \\sim \\pi} \\left[ Q ^ {\\pi} (s, a) \\nabla_w \\log{\\pi(a | s; w)}\\right] = \\mathop{\\mathbb{E}}_{s \\sim d ^ {\\pi}} \\left[ \\sum_{a \\in \\mathcal{A}} \\pi(a | s; w) Q ^ {\\pi} (s, a) \\nabla_w \\log{\\pi(a | s; w)}\\right] = \\mathop{\\mathbb{E}}_{s \\sim d ^ {\\pi}} \\left[ \\sum_{a \\in \\mathcal{A}} \\nabla_w \\pi(a | s; w) Q ^ {\\pi} (s, a) \\right] \\approx \\\\ &\\approx \\mathop{\\mathbb{E}}_{s \\sim d ^ {\\pi}} \\left[ \\nabla_w \\sum_{a \\in \\mathcal{A}} \\pi(a | s; w) \\hat{Q} (s, a; u) \\right]\n",
    "\\end{align} $$\n",
    "Таким образом при обучении параметры $w$ подстраиваются так, чтобы максимизировать $\\sum_{a \\in \\mathcal{A}} \\pi(a | s; w) \\hat{Q} (s, a; u) = \\tilde{V}(s; w, u)$ - оценку полезности состояний, рассчитанную по текущей оценке полезности действий $\\hat{Q}$. При этом действия из траекторий $d ^ {\\pi}$ явным образом в $\\nabla J$ не входят.\n",
    "\n",
    "# Задание\n",
    "1. **Реализовать предложенную схему on-policy обучения актора-критика.** За основу можно взять Advantage Actor Critic (A2C) (см. реализация ниже). Модель критика $\\hat{Q}(s, a; u)$ аппроксимирует полезность действий (см. семинар DQN) и обучается с помощью n-шаговых отдач. Оптимизационная задача для актора $\\pi(s, a; w)$ : $\\sum_{a \\in \\mathcal{A}} \\pi(a | s; w) \\text{ stop-gradient}[\\hat{Q} (s, a; u)] \\rightarrow \\max_\\limits{w}$. Сравнить результаты реализованного алгоритма с A2C.\n",
    "2. **Исследовать влияние базового уровня на реализованный алгоритм.** Использовать функцию преимущества $A(s, a) = Q(s, a) - V(s)$ для расчёта градиента стратегии: $\\sum_{a \\in \\mathcal{A}} \\pi(a | s; w) \\text{ stop-gradient}[\\hat{A}(s, a)] \\rightarrow \\max_\\limits{w}$, где $\\hat{A}(s, a) = \\hat{Q}(s, a; u) - \\tilde{V}(s; w, u) = \\hat{Q}(s, a; u) - \\sum_{a \\in \\mathcal{A}} \\pi(a | s; w) \\hat{Q} (s, a; u) $\n",
    "\n",
    "В качестве среды взять задачу PongNoFrameskip-v4 из набора сред [Atari](https://ale.farama.org/environments/).\n",
    "\n",
    "В отчёте на одном графике должны быть представлены кривые эпмирических зависимостей отдачи, усреднённой по 60 eval-эпизодам и двум запускам обучения алгоритма (т.е. 30 эпизодов для одного запуска) для:\n",
    "- A2C\n",
    "- алгоритма, реализующего задание 1\n",
    "- алгоритма, реализующего задание 2\n",
    "\n",
    "Также в отчёте должен быть параграф с анализом результатов.\n",
    "\n",
    "#### Дополнительная информация\n",
    "- Обучение можно остановить при достижении отдачи >18 (обычно хватает 15 млн шагов, среднее время обучения в google colab - 10 часов)\n",
    "- Гиперпарамтры для A2C указаны в примере\n",
    "- Гиперпараметры для алгоритмов, реализующих задания 1 и 2: `pg_coef=0.1`, `vf_coef=0.5`, `ent_coef=0.001`, `lr=0.0015` (другие гиперпараметры совпадают с A2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Пример работы в google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G2L0DtvNVsLW",
    "outputId": "306c0cb8-0856-4b8e-89d6-7785cd9d8cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "    if COLAB:\n",
    "        ! pip install 'gymnasium[atari,accept-rom-license]==0.29.1' wandb==0.19.1 && pip install --no-deps stable-baselines3==2.3.2\n",
    "except:\n",
    "    COLAB = False\n",
    "\n",
    "NEED_DRIVE = True\n",
    "HAS_DRIVE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLNiry6C4VIU"
   },
   "source": [
    "### При обучении в google colab чек-поинт необходимо сохранять в google drive, чтобы не потерять данные при отключении тетрадки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37Id9af9aVVZ",
    "outputId": "4176be00-01de-457b-efe7-2275c776f484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "if NEED_DRIVE and COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    HAS_DRIVE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ScVmcqJjaiMn"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "from typing import Dict, Type, Any, Tuple, NamedTuple, Optional, Union, Generator, List\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium.spaces as spaces\n",
    "from gymnasium.vector import AsyncVectorEnv, SyncVectorEnv\n",
    "from gymnasium.wrappers import AtariPreprocessing, RecordEpisodeStatistics, FrameStack\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "from stable_baselines3.common.sb2_compat.rmsprop_tf_like import RMSpropTFLike\n",
    "\n",
    "import wandb\n",
    "os.environ[\"WANDB_API_KEY\"] = \"your wandb api key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hs-1UAZfaVVa"
   },
   "source": [
    "#### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-Ul9_PJ9gMvA"
   },
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed: int, using_cuda: bool = False) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if using_cuda:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMyWS3dXdtQM"
   },
   "source": [
    "## Environment\n",
    "Используем векторизованные среды: агент взаимодействует сразу с `n_envs` экземлярами среды. Таким образом в буфер собираются данные из `n_envs` эпизодов, повышая разнообразие данных, на которых обучается стратегия. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "isaErUYAWHTJ"
   },
   "outputs": [],
   "source": [
    "def make_env(n_envs, env_id):\n",
    "    def _make():\n",
    "        env = gym.make(env_id)\n",
    "        env = RecordEpisodeStatistics(env)\n",
    "        env = AtariPreprocessing(env)\n",
    "        env = FrameStack(env, num_stack=4)\n",
    "\n",
    "        return env\n",
    "\n",
    "    vec_env = AsyncVectorEnv([_make for _ in range(n_envs)])\n",
    "\n",
    "    return vec_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMNka5o-dxwE"
   },
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oJ37EN_MWmun"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            observation_space: gym.Space,\n",
    "            features_dim: int = 512,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.features_dim = features_dim\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(torch.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self,\n",
    "            observation_space: spaces.Space,\n",
    "            action_space: spaces.Space,\n",
    "            lr: float,):\n",
    "        super().__init__()\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.lr = lr\n",
    "        self.features_extractor = self.make_features_extractor()\n",
    "        self.features_dim = self.features_extractor.features_dim\n",
    "        self.action_net = nn.Linear(self.features_dim, self.action_space.n)\n",
    "        self.value_net = nn.Linear(self.features_dim, 1)\n",
    "\n",
    "        self.optimizer = RMSpropTFLike(self.parameters(), lr=self.lr, eps=1e-5)\n",
    "\n",
    "    def make_features_extractor(self):\n",
    "        return Encoder(self.observation_space)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, deterministic: bool = False) -> Dict[str, torch.Tensor]:\n",
    "        features = self.extract_features(obs)\n",
    "        action_logits = self.action_net(features)\n",
    "        distribution = Categorical(logits=action_logits)\n",
    "        actions = distribution.mode() if deterministic else distribution.sample()\n",
    "        policy_output = {'actions': actions, 'log_probs': distribution.log_prob(actions)}\n",
    "\n",
    "        values = self.value_net(features)\n",
    "        policy_output['values'] = values\n",
    "\n",
    "        return policy_output\n",
    "\n",
    "    def extract_features(self, obs: torch.Tensor):\n",
    "        obs = obs.float() / 255.\n",
    "        features = self.features_extractor(obs)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def evaluate_actions(self, obs: torch.Tensor, actions_taken: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        features = self.extract_features(obs)\n",
    "        action_logits = self.action_net(features)\n",
    "        distribution = Categorical(logits=action_logits)\n",
    "        log_probs = distribution.log_prob(actions_taken)\n",
    "        entropy = distribution.entropy()\n",
    "        values = self.value_net(features)\n",
    "\n",
    "        return {'log_probs': log_probs, 'values': values, 'entropy': entropy}\n",
    "\n",
    "    def predict_values(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward(obs)['values']\n",
    "\n",
    "    def set_training_mode(self, mode: bool) -> None:\n",
    "        self.train(mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6E9af0DeWt9"
   },
   "source": [
    "## Buffer\n",
    "Версия буфера, где хранятся данные из `n_envs` сред."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EQR5XfMfeVeS"
   },
   "outputs": [],
   "source": [
    "class BufferSamples(NamedTuple):\n",
    "    observations: torch.Tensor\n",
    "    actions: torch.Tensor\n",
    "    values: torch.Tensor\n",
    "    log_prob: torch.Tensor\n",
    "    advantages: torch.Tensor\n",
    "    returns: torch.Tensor\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            device: Union[torch.device, str] = \"auto\",\n",
    "            gamma: float = 0.99,\n",
    "            n_envs: int = 1,\n",
    "    ):\n",
    "        self.device = torch.device(device)\n",
    "        self.n_envs = n_envs\n",
    "        self.gamma = gamma\n",
    "        self.observations = None\n",
    "        self.actions = None\n",
    "        self.rewards = None\n",
    "        self.returns = None\n",
    "        self.values = None\n",
    "        self.log_probs = None\n",
    "        self.episode_starts = None\n",
    "        self.advantages = None\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.observations = [[] for _ in range(self.n_envs)]\n",
    "        self.actions = [[] for _ in range(self.n_envs)]\n",
    "        self.rewards = [[] for _ in range(self.n_envs)]\n",
    "        self.returns = [None for _ in range(self.n_envs)]\n",
    "        self.episode_starts = [[] for _ in range(self.n_envs)]\n",
    "        self.values = [[] for _ in range(self.n_envs)]\n",
    "        self.log_probs = [[] for _ in range(self.n_envs)]\n",
    "        self.advantages = [None for _ in range(self.n_envs)]\n",
    "\n",
    "    def size(self) -> int:\n",
    "        return sum(len(env_actions) for env_actions in self.actions)\n",
    "\n",
    "    @staticmethod\n",
    "    def flatten(buffer: list, dtype=None) -> np.ndarray:\n",
    "        data = []\n",
    "        for env_data in buffer:\n",
    "            data.extend(env_data)\n",
    "\n",
    "        return np.stack(data).astype(dtype)\n",
    "\n",
    "    def add(\n",
    "            self,\n",
    "            obs: np.ndarray,\n",
    "            action: np.ndarray,\n",
    "            reward: np.ndarray,\n",
    "            episode_start: np.ndarray,\n",
    "            value: torch.Tensor,\n",
    "            log_prob: torch.Tensor,\n",
    "    ) -> None:\n",
    "        if len(log_prob.shape) == 0:\n",
    "            # Reshape 0-d tensor to avoid error\n",
    "            log_prob = log_prob.reshape(-1, 1)\n",
    "\n",
    "        log_prob = log_prob.clone().cpu().numpy()\n",
    "        value = value.clone().flatten().cpu().numpy()\n",
    "        for env_id in range(self.n_envs):\n",
    "            self.observations[env_id].append(obs[env_id])\n",
    "            self.actions[env_id].append(action[env_id])\n",
    "            self.rewards[env_id].append(reward[env_id])\n",
    "            self.episode_starts[env_id].append(episode_start[env_id])\n",
    "            self.values[env_id].append(value[env_id])\n",
    "            self.log_probs[env_id].append(log_prob[env_id])\n",
    "\n",
    "    def to_torch(self, array: np.ndarray) -> torch.Tensor:\n",
    "        return torch.tensor(array, device=self.device)\n",
    "\n",
    "    def get(self) -> BufferSamples:\n",
    "        self.observations = self.flatten(self.observations)\n",
    "        self.actions = self.flatten(self.actions)\n",
    "        self.advantages = self.flatten(self.advantages, dtype=np.float32)\n",
    "        self.returns = self.flatten(self.returns, dtype=np.float32)\n",
    "        self.log_probs = self.flatten(self.log_probs, dtype=np.float32)\n",
    "        self.values = self.flatten(self.values, dtype=np.float32)\n",
    "\n",
    "        data = (\n",
    "            self.observations,\n",
    "            self.actions,\n",
    "            self.values,\n",
    "            self.log_probs,\n",
    "            self.advantages.reshape(-1),\n",
    "            self.returns.reshape(-1),\n",
    "        )\n",
    "\n",
    "        return BufferSamples(*tuple(map(self.to_torch, data)))\n",
    "\n",
    "    def compute_returns_and_advantage(self, last_values: torch.Tensor, dones: np.ndarray) -> None:\n",
    "        last_values = last_values.flatten().cpu().numpy()\n",
    "        for env_id in range(self.n_envs):\n",
    "            values = self.values[env_id]\n",
    "            actions = self.actions[env_id]\n",
    "            rewards = self.rewards[env_id]\n",
    "            episode_starts = self.episode_starts[env_id]\n",
    "            assert len(values) == len(actions)\n",
    "            assert len(rewards) == len(actions)\n",
    "            assert len(episode_starts) == len(actions)\n",
    "\n",
    "            returns = [None] * len(rewards)\n",
    "            advantages = [None] * len(rewards)\n",
    "            episode_starts.append(dones[env_id])\n",
    "            reward_to_go = last_values[env_id]\n",
    "            for step in reversed(range(len(rewards))):\n",
    "                is_next_step_terminal = episode_starts[step + 1]\n",
    "                reward_to_go = 0 if is_next_step_terminal else reward_to_go\n",
    "                reward_to_go = rewards[step] + self.gamma * reward_to_go\n",
    "                returns[step] = reward_to_go\n",
    "                advantages[step] = reward_to_go - values[step]\n",
    "\n",
    "            self.returns[env_id] = returns\n",
    "            self.advantages[env_id] = advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83GHD_8i7QPz"
   },
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-_5IS_767Sb7"
   },
   "outputs": [],
   "source": [
    "class A2C:\n",
    "    def __init__(\n",
    "            self,\n",
    "            env: gym.vector.VectorEnv,\n",
    "            lr: float = 7e-4,\n",
    "            n_steps: int = 5,\n",
    "            gamma: float = 0.99,\n",
    "            ent_coef: float = 0.01,\n",
    "            vf_coef: float = 0.25,\n",
    "            pg_coef: float = 1.0,\n",
    "            max_grad_norm: float = 0.5,\n",
    "            stats_window_size: int = 100,\n",
    "            seed: Optional[int] = None,\n",
    "            device: Union[torch.device, str] = \"cuda\",\n",
    "            eval_env=None,\n",
    "            n_eval_episodes=None,\n",
    "            eval_interval=None,\n",
    "    ):\n",
    "        self.device = device if isinstance(device, torch.device) else torch.device(device)\n",
    "        print(f\"Using {self.device} device\")\n",
    "\n",
    "        self.num_timesteps = 0\n",
    "        self._num_timesteps_at_start = 0\n",
    "        self._episode_num = 0\n",
    "        self.seed = seed\n",
    "        self.start_time = None\n",
    "        self.lr = lr\n",
    "        self._last_obs = None\n",
    "        self._last_episode_starts = None\n",
    "        self._stats_window_size = stats_window_size\n",
    "        self.ep_info_buffer = deque(maxlen=self._stats_window_size)\n",
    "        self._n_updates = 0\n",
    "\n",
    "        self.observation_space = env.single_observation_space\n",
    "        self.action_space = env.single_action_space\n",
    "        self.n_envs = env.num_envs\n",
    "        self.env = env\n",
    "        self.eval_env = eval_env\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self.eval_interval = eval_interval or math.inf\n",
    "        self.next_eval_step = self.eval_interval\n",
    "\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.ent_coef = ent_coef\n",
    "        self.vf_coef = vf_coef\n",
    "        self.pg_coef = pg_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.log_dict = {}\n",
    "        self.evaluation_history = []\n",
    "        self.checkpoint_path = None\n",
    "\n",
    "        self.set_random_seed(self.seed)\n",
    "        self.rollout_buffer = Buffer(\n",
    "            device=self.device,\n",
    "            gamma=self.gamma,\n",
    "            n_envs=self.n_envs,\n",
    "        )\n",
    "        self.policy = Policy(\n",
    "            self.observation_space, self.action_space, self.lr\n",
    "        )\n",
    "        self.policy = self.policy.to(self.device)\n",
    "\n",
    "    def _update_info_buffer(self, infos: Dict[str, Any]) -> None:\n",
    "        assert self.ep_info_buffer is not None\n",
    "\n",
    "        if 'final_info' not in infos:\n",
    "            return\n",
    "\n",
    "        for idx, info in enumerate(infos['final_info']):\n",
    "            if info is not None:\n",
    "                maybe_ep_info = info.get(\"episode\")\n",
    "                if maybe_ep_info is not None:\n",
    "                    self.ep_info_buffer.extend([maybe_ep_info])\n",
    "\n",
    "    def set_random_seed(self, seed: Optional[int] = None) -> None:\n",
    "        if seed is None:\n",
    "            return\n",
    "        set_seed_everywhere(seed, using_cuda=self.device.type == torch.device(\"cuda\").type)\n",
    "        self.action_space.seed(seed)\n",
    "        self.observation_space.seed(seed)\n",
    "\n",
    "    def collect_rollouts(\n",
    "            self,\n",
    "            env,\n",
    "            buffer: Buffer,\n",
    "    ):\n",
    "        self.policy.set_training_mode(False)\n",
    "        assert self._last_obs is not None, \"No previous observation was provided\"\n",
    "\n",
    "        step = 0\n",
    "        buffer.reset()\n",
    "        while step < self.n_steps:\n",
    "            with torch.no_grad():\n",
    "                obs_tensor = torch.as_tensor(self._last_obs, device=self.device)\n",
    "                policy_result = self.policy(obs_tensor)\n",
    "                actions, values, log_probs = policy_result['actions'], policy_result['values'], policy_result[\n",
    "                    'log_probs']\n",
    "            actions = actions.cpu().numpy()\n",
    "            new_obs, rewards, terminateds, truncateds, infos = env.step(actions)\n",
    "\n",
    "            self.num_timesteps += env.num_envs\n",
    "            self._update_info_buffer(infos)\n",
    "            dones = terminateds | truncateds\n",
    "            self._episode_num += np.sum(dones).item()\n",
    "            actions = actions.reshape(-1, 1)\n",
    "\n",
    "            # Handle timeout by bootstraping with value function\n",
    "            for idx, truncated in enumerate(truncateds):\n",
    "                if (\n",
    "                        truncated\n",
    "                        and \"final_observation\" in infos\n",
    "                        and infos[\"final_observation\"][idx] is not None\n",
    "                ):\n",
    "                    terminal_obs = torch.as_tensor(infos[\"final_observation\"][idx], device=self.device)\n",
    "                    with torch.no_grad():\n",
    "                        terminal_value = self.policy.predict_values(terminal_obs.unsqueeze(0)).item()\n",
    "                    rewards[idx] += self.gamma * terminal_value\n",
    "\n",
    "            buffer.add(\n",
    "                self._last_obs,  # type: ignore[arg-type]\n",
    "                actions,\n",
    "                rewards,\n",
    "                self._last_episode_starts,  # type: ignore[arg-type]\n",
    "                values,\n",
    "                log_probs,\n",
    "            )\n",
    "            self._last_obs = new_obs  # type: ignore[assignment]\n",
    "            self._last_episode_starts = dones\n",
    "\n",
    "            if self.num_timesteps >= self.next_eval_step:\n",
    "                self.evaluate()\n",
    "                self.next_eval_step += self.eval_interval\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            values = self.policy.predict_values(torch.as_tensor(np.array(new_obs), device=self.device))\n",
    "\n",
    "        buffer.compute_returns_and_advantage(last_values=values, dones=dones)\n",
    "\n",
    "    def train(self) -> None:\n",
    "        self.policy.set_training_mode(True)\n",
    "\n",
    "        rollout_data = self.rollout_buffer.get()\n",
    "        actions = rollout_data.actions.long().flatten()\n",
    "        policy_output = self.policy.evaluate_actions(rollout_data.observations, actions)\n",
    "        log_prob = policy_output['log_probs']\n",
    "        values = policy_output['values']\n",
    "        entropy = policy_output['entropy']\n",
    "        advantages = rollout_data.advantages\n",
    "\n",
    "        # Policy gradient loss\n",
    "        policy_loss = -self.pg_coef * (advantages * log_prob).mean()\n",
    "\n",
    "        # Value loss\n",
    "        value_loss = self.vf_coef * F.mse_loss(values, rollout_data.returns.reshape_as(values))\n",
    "\n",
    "        # Entropy loss\n",
    "        entropy = torch.mean(entropy)\n",
    "        entropy_loss = -self.ent_coef * entropy\n",
    "\n",
    "        loss = policy_loss + entropy_loss + value_loss\n",
    "\n",
    "        # Optimization step\n",
    "        self.policy.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "        self.policy.optimizer.step()\n",
    "        self._n_updates += 1\n",
    "\n",
    "        self.log_dict = dict([\n",
    "            (\"train/entropy_loss\", entropy_loss.item()), (\"train/policy_gradient_loss\", policy_loss.item()),\n",
    "            (\"train/value_loss\", value_loss.item()), (\"train/loss\", loss.item()),\n",
    "            (\"train/grad_norm\", grad_norm.item()), (\"train/n_updates\", self._n_updates),\n",
    "            (\"train/entropy\", entropy.item()),\n",
    "        ])\n",
    "\n",
    "    def _dump_logs(self, iteration: int) -> None:\n",
    "        assert self.ep_info_buffer is not None\n",
    "\n",
    "        time_elapsed = max((time.time_ns() - self.start_time) / 1e9, sys.float_info.epsilon)\n",
    "        fps = int((self.num_timesteps - self._num_timesteps_at_start) / time_elapsed)\n",
    "        self.log_dict[\"time/iterations\"] = iteration\n",
    "        if len(self.ep_info_buffer) > 0 and len(self.ep_info_buffer[0]) > 0:\n",
    "            self.log_dict[\"rollout/ep_rew_mean\"] = np.mean(\n",
    "                np.asarray([ep_info[\"r\"] for ep_info in self.ep_info_buffer])).item()\n",
    "            self.log_dict[\"rollout/ep_len_mean\"] = np.mean(\n",
    "                np.asarray([ep_info[\"l\"] for ep_info in self.ep_info_buffer])).item()\n",
    "\n",
    "        self.log_dict[\"time/fps\"] = fps\n",
    "        self.log_dict[\"time/time_elapsed\"] = int(time_elapsed)\n",
    "        self.log_dict[\"time/total_timesteps\"] = self.num_timesteps\n",
    "        self.log_dict[\"time/total_episodes\"] = self._episode_num\n",
    "        print(json.dumps(self.log_dict, sort_keys=True, indent=4), flush=True)\n",
    "        if wandb.run is not None:\n",
    "            wandb.log(step=self.num_timesteps, data=self.log_dict)\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.policy.set_training_mode(False)\n",
    "        if self.eval_env is None or self.n_eval_episodes is None:\n",
    "            return\n",
    "\n",
    "        n_eval_episodes = self.n_eval_episodes\n",
    "        eval_env = self.eval_env\n",
    "        n_envs = eval_env.num_envs\n",
    "        returns = []\n",
    "        lengths = []\n",
    "        episode_counts = np.zeros(n_envs, dtype=np.int32)\n",
    "        episode_count_targets = np.array([(n_eval_episodes + i) // n_envs for i in range(n_envs)], dtype=np.int32)\n",
    "        observations, _ = eval_env.reset()\n",
    "        while (episode_counts < episode_count_targets).any():\n",
    "            with torch.no_grad():\n",
    "                obs_tensor = torch.as_tensor(np.array(observations), device=self.device)\n",
    "                actions = self.policy(obs_tensor)['actions']\n",
    "            actions = actions.cpu().numpy()\n",
    "            new_observations, rewards, terminateds, truncateds, infos = eval_env.step(actions)\n",
    "            dones = terminateds | truncateds\n",
    "\n",
    "            for i in range(n_envs):\n",
    "                if episode_counts[i] < episode_count_targets[i]:\n",
    "                    if dones[i]:\n",
    "                        assert 'final_info' in infos, 'Evaluation environments must be wrapped into RecordEpisodeStatistics'\n",
    "                        info = infos['final_info'][i]\n",
    "                        returns.append(info['episode']['r'].item())\n",
    "                        lengths.append(info['episode']['l'].item())\n",
    "                        episode_counts[i] += 1\n",
    "\n",
    "            observations = new_observations\n",
    "\n",
    "        assert len(returns) == n_eval_episodes\n",
    "        assert len(lengths) == n_eval_episodes\n",
    "\n",
    "        self.evaluation_history.append({'step': self.num_timesteps, 'returns': returns, 'lengths': lengths})\n",
    "        self.save_checkpoint()\n",
    "\n",
    "        log_eval_dict = {\n",
    "            'time/total_timesteps': self.num_timesteps,\n",
    "            'eval/return': np.mean(returns),\n",
    "            'eval/length': np.mean(lengths),\n",
    "        }\n",
    "        if wandb.run is not None:\n",
    "            wandb.log(step=self.num_timesteps, data=log_eval_dict)\n",
    "\n",
    "        log_eval_dict_stdout = {\n",
    "            'time/total_timesteps': self.num_timesteps,\n",
    "            'eval/return': f'{np.mean(returns)} +/- {np.std(returns, ddof=1) / np.sqrt(len(returns))}',\n",
    "            'eval/length': f'{np.mean(lengths)} +/- {np.std(lengths, ddof=1) / np.sqrt(len(lengths))}',\n",
    "        }\n",
    "\n",
    "        print(json.dumps(log_eval_dict_stdout, sort_keys=True, indent=4))\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        if self.checkpoint_path is None:\n",
    "            print('Skip checkpoint saving')\n",
    "            return\n",
    "\n",
    "        checkpoint = {\n",
    "            'policy': self.policy.state_dict(), 'optimizer': self.policy.optimizer.state_dict(),\n",
    "            'num_timesteps': self.num_timesteps, '_episode_num': self._episode_num, '_n_updates': self._n_updates,\n",
    "            'ep_info_buffer': self.ep_info_buffer, 'evaluation_history': self.evaluation_history,\n",
    "        }\n",
    "        os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "        torch.save(checkpoint, os.path.join(self.checkpoint_path, 'checkpoint.pt'))\n",
    "        print(f'Save checkpoint to {self.checkpoint_path}')\n",
    "\n",
    "    def load_checkpoint(self, load_checkpoint_path):\n",
    "        print(f'Loading checkpoint from folder: {load_checkpoint_path}')\n",
    "        file_path = os.path.join(load_checkpoint_path, 'checkpoint.pt')\n",
    "        checkpoint = torch.load(file_path)\n",
    "        self.policy.load_state_dict(checkpoint['policy'])\n",
    "        self.policy.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.num_timesteps = checkpoint['num_timesteps']\n",
    "        self._num_timesteps_at_start = self.num_timesteps\n",
    "        self._episode_num = checkpoint['_episode_num']\n",
    "        self._n_updates = checkpoint['_n_updates']\n",
    "        self.ep_info_buffer = checkpoint['ep_info_buffer']\n",
    "        self.evaluation_history = checkpoint['evaluation_history']\n",
    "        self.next_eval_step = self.evaluation_history[-1]['step'] + self.eval_interval\n",
    "        print(f'Resume from step: {self.num_timesteps}')\n",
    "\n",
    "    def learn(\n",
    "            self,\n",
    "            total_timesteps: int,\n",
    "            log_interval: int = 1,\n",
    "            checkpoint_path: str = None,\n",
    "    ):\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        assert self.env is not None\n",
    "\n",
    "        self.start_time = time.time_ns()\n",
    "\n",
    "        if self._last_obs is None:\n",
    "            self._last_obs, _ = self.env.reset(seed=self.seed)\n",
    "            self._last_episode_starts = np.ones((self.env.num_envs,), dtype=bool)\n",
    "\n",
    "            if self.eval_env is not None:\n",
    "                self.eval_env.reset(seed=self.seed + self.n_envs)\n",
    "\n",
    "        iteration = 0\n",
    "        while self.num_timesteps < total_timesteps:\n",
    "            self.collect_rollouts(self.env, self.rollout_buffer)\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            if log_interval is not None and iteration % log_interval == 0:\n",
    "                assert self.ep_info_buffer is not None\n",
    "                self._dump_logs(iteration)\n",
    "\n",
    "            self.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIMvVG--afEA"
   },
   "source": [
    "# Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ar9b_XcYaVVh"
   },
   "outputs": [],
   "source": [
    "def run_algorithm(\n",
    "    save_checkpoint_path, project, run_name, seed, eval_interval, total_timesteps,\n",
    "    resume_checkpoint_path=None, resume_run_id=None):\n",
    "    run = wandb.init(\n",
    "        project=project,\n",
    "        name=f'{run_name}',\n",
    "        resume='never' if resume_run_id is None else 'must',\n",
    "        id=resume_run_id,\n",
    "    )\n",
    "\n",
    "    env = make_env(env_id='PongNoFrameskip-v4', n_envs=16)\n",
    "    eval_env = make_env(env_id='PongNoFrameskip-v4', n_envs=30)\n",
    "\n",
    "    model = A2C(\n",
    "        env=env, lr=0.0007, n_steps=5,\n",
    "        vf_coef=0.25, ent_coef=0.01, pg_coef=1,\n",
    "        seed=seed,\n",
    "        eval_env=eval_env, n_eval_episodes=30, eval_interval=eval_interval,\n",
    "    )\n",
    "\n",
    "    if resume_checkpoint_path is not None:\n",
    "        model.load_checkpoint(resume_checkpoint_path)\n",
    "\n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,\n",
    "        log_interval=100,\n",
    "        checkpoint_path=save_checkpoint_path,\n",
    "    )\n",
    "\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-91_vyUT2S3j"
   },
   "source": [
    "### Пример сохранения чек-поинта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VWQAWWWxaVVh",
    "outputId": "eb06adda-f433-4a91-f747-a6fbb98a09a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py:202: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  settings = self._wl.settings.copy()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mula_elfray\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250312_151100-nij2ao0p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ula_elfray/Homework_ActorCritic/runs/nij2ao0p' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/ula_elfray/Homework_ActorCritic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ula_elfray/Homework_ActorCritic' target=\"_blank\">https://wandb.ai/ula_elfray/Homework_ActorCritic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ula_elfray/Homework_ActorCritic/runs/nij2ao0p' target=\"_blank\">https://wandb.ai/ula_elfray/Homework_ActorCritic/runs/nij2ao0p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "{\n",
      "    \"time/fps\": 339,\n",
      "    \"time/iterations\": 100,\n",
      "    \"time/time_elapsed\": 23,\n",
      "    \"time/total_episodes\": 0,\n",
      "    \"time/total_timesteps\": 8000,\n",
      "    \"train/entropy\": 1.791576623916626,\n",
      "    \"train/entropy_loss\": -0.017915766686201096,\n",
      "    \"train/grad_norm\": 0.02656707726418972,\n",
      "    \"train/loss\": -0.05745035782456398,\n",
      "    \"train/n_updates\": 99,\n",
      "    \"train/policy_gradient_loss\": -0.045731063932180405,\n",
      "    \"train/value_loss\": 0.0061964732594788074\n",
      "}\n",
      "Save checkpoint to /content/drive/My Drive/rl_homework/a2c/test/1\n",
      "{\n",
      "    \"eval/length\": \"3770.5333333333333 +/- 94.27174871277175\",\n",
      "    \"eval/return\": \"-20.033333333333335 +/- 0.22223180055985828\",\n",
      "    \"time/total_timesteps\": 9008\n",
      "}\n",
      "{\n",
      "    \"rollout/ep_len_mean\": 3351.75,\n",
      "    \"rollout/ep_rew_mean\": -20.75,\n",
      "    \"time/fps\": 171,\n",
      "    \"time/iterations\": 200,\n",
      "    \"time/time_elapsed\": 93,\n",
      "    \"time/total_episodes\": 12,\n",
      "    \"time/total_timesteps\": 16000,\n",
      "    \"train/entropy\": 1.7915773391723633,\n",
      "    \"train/entropy_loss\": -0.017915772274136543,\n",
      "    \"train/grad_norm\": 0.05245092883706093,\n",
      "    \"train/loss\": -0.113218292593956,\n",
      "    \"train/n_updates\": 199,\n",
      "    \"train/policy_gradient_loss\": -0.11032344400882721,\n",
      "    \"train/value_loss\": 0.01502092182636261\n",
      "}\n",
      "Save checkpoint to /content/drive/My Drive/rl_homework/a2c/test/1\n",
      "{\n",
      "    \"eval/length\": \"3628.4333333333334 +/- 75.92900015892545\",\n",
      "    \"eval/return\": \"-20.266666666666666 +/- 0.13504646568022544\",\n",
      "    \"time/total_timesteps\": 18000\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/length</td><td>█▁</td></tr><tr><td>eval/return</td><td>█▁</td></tr><tr><td>rollout/ep_len_mean</td><td>▁</td></tr><tr><td>rollout/ep_rew_mean</td><td>▁</td></tr><tr><td>time/fps</td><td>█▁</td></tr><tr><td>time/iterations</td><td>▁█</td></tr><tr><td>time/time_elapsed</td><td>▁█</td></tr><tr><td>time/total_episodes</td><td>▁█</td></tr><tr><td>time/total_timesteps</td><td>▁▂▇█</td></tr><tr><td>train/entropy</td><td>▁█</td></tr><tr><td>train/entropy_loss</td><td>█▁</td></tr><tr><td>train/grad_norm</td><td>▁█</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/n_updates</td><td>▁█</td></tr><tr><td>train/policy_gradient_loss</td><td>█▁</td></tr><tr><td>train/value_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/length</td><td>3628.43333</td></tr><tr><td>eval/return</td><td>-20.26667</td></tr><tr><td>rollout/ep_len_mean</td><td>3351.75</td></tr><tr><td>rollout/ep_rew_mean</td><td>-20.75</td></tr><tr><td>time/fps</td><td>171</td></tr><tr><td>time/iterations</td><td>200</td></tr><tr><td>time/time_elapsed</td><td>93</td></tr><tr><td>time/total_episodes</td><td>12</td></tr><tr><td>time/total_timesteps</td><td>18000</td></tr><tr><td>train/entropy</td><td>1.79158</td></tr><tr><td>train/entropy_loss</td><td>-0.01792</td></tr><tr><td>train/grad_norm</td><td>0.05245</td></tr><tr><td>train/loss</td><td>-0.11322</td></tr><tr><td>train/n_updates</td><td>199</td></tr><tr><td>train/policy_gradient_loss</td><td>-0.11032</td></tr><tr><td>train/value_loss</td><td>0.01502</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test</strong> at: <a href='https://wandb.ai/ula_elfray/Homework_ActorCritic/runs/nij2ao0p' target=\"_blank\">https://wandb.ai/ula_elfray/Homework_ActorCritic/runs/nij2ao0p</a><br> View project at: <a href='https://wandb.ai/ula_elfray/Homework_ActorCritic' target=\"_blank\">https://wandb.ai/ula_elfray/Homework_ActorCritic</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250312_151100-nij2ao0p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_algorithm(\n",
    "    save_checkpoint_path='/content/drive/My Drive/rl_homework/a2c/test/1',\n",
    "    project='Homework_ActorCritic', run_name='test', seed=0,\n",
    "    eval_interval=9000, total_timesteps=20000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-08k_k32S3j"
   },
   "source": [
    "### Возобновление обучения из чек-поинта\n",
    "resume_run_id - последний сегмент в URL запуска, который хотим возобновить:\n",
    "\n",
    "`https://wandb.ai/ula_elfray/Homework_ActorCritic/runs/`**nij2ao0p**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8GUMfkrAaVVh",
    "outputId": "7cebae09-5cbe-44cb-fb5c-906f8a1b5d6d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py:202: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  settings = self._wl.settings.copy()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250312_151428-nij2ao0p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/ula_elfray/Homework_ActorCritic/runs/nij2ao0p' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/ula_elfray/Homework_ActorCritic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ula_elfray/Homework_ActorCritic' target=\"_blank\">https://wandb.ai/ula_elfray/Homework_ActorCritic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ula_elfray/Homework_ActorCritic/runs/nij2ao0p' target=\"_blank\">https://wandb.ai/ula_elfray/Homework_ActorCritic/runs/nij2ao0p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Loading checkpoint from folder: /content/drive/My Drive/rl_homework/a2c/test/1\n",
      "Resume from step: 18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-4d861f853964>:270: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"rollout/ep_len_mean\": 3454.0714285714284,\n",
      "    \"rollout/ep_rew_mean\": -20.571428298950195,\n",
      "    \"time/fps\": 340,\n",
      "    \"time/iterations\": 100,\n",
      "    \"time/time_elapsed\": 23,\n",
      "    \"time/total_episodes\": 14,\n",
      "    \"time/total_timesteps\": 26000,\n",
      "    \"train/entropy\": 1.7915782928466797,\n",
      "    \"train/entropy_loss\": -0.01791578345000744,\n",
      "    \"train/grad_norm\": 0.04131273552775383,\n",
      "    \"train/loss\": 0.05740572512149811,\n",
      "    \"train/n_updates\": 323,\n",
      "    \"train/policy_gradient_loss\": 0.0661173090338707,\n",
      "    \"train/value_loss\": 0.009204201400279999\n",
      "}\n",
      "Save checkpoint to /content/drive/My Drive/rl_homework/a2c/test/2\n",
      "{\n",
      "    \"eval/length\": \"3725.4333333333334 +/- 64.68572446986715\",\n",
      "    \"eval/return\": \"-20.2 +/- 0.13896166675593022\",\n",
      "    \"time/total_timesteps\": 27008\n",
      "}\n",
      "{\n",
      "    \"rollout/ep_len_mean\": 3512.4444444444443,\n",
      "    \"rollout/ep_rew_mean\": -20.592592239379883,\n",
      "    \"time/fps\": 179,\n",
      "    \"time/iterations\": 200,\n",
      "    \"time/time_elapsed\": 89,\n",
      "    \"time/total_episodes\": 27,\n",
      "    \"time/total_timesteps\": 34000,\n",
      "    \"train/entropy\": 1.7915823459625244,\n",
      "    \"train/entropy_loss\": -0.017915822565555573,\n",
      "    \"train/grad_norm\": 0.06179024651646614,\n",
      "    \"train/loss\": -0.10954311490058899,\n",
      "    \"train/n_updates\": 423,\n",
      "    \"train/policy_gradient_loss\": -0.10661293566226959,\n",
      "    \"train/value_loss\": 0.014985642395913601\n",
      "}\n",
      "Save checkpoint to /content/drive/My Drive/rl_homework/a2c/test/2\n",
      "{\n",
      "    \"eval/length\": \"3778.8333333333335 +/- 99.92864216481784\",\n",
      "    \"eval/return\": \"-20.233333333333334 +/- 0.21271822241076074\",\n",
      "    \"time/total_timesteps\": 36000\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/length</td><td>▁█</td></tr><tr><td>eval/return</td><td>█▁</td></tr><tr><td>rollout/ep_len_mean</td><td>▁█</td></tr><tr><td>rollout/ep_rew_mean</td><td>█▁</td></tr><tr><td>time/fps</td><td>█▁</td></tr><tr><td>time/iterations</td><td>▁█</td></tr><tr><td>time/time_elapsed</td><td>▁█</td></tr><tr><td>time/total_episodes</td><td>▁█</td></tr><tr><td>time/total_timesteps</td><td>▁▂▇█</td></tr><tr><td>train/entropy</td><td>▁█</td></tr><tr><td>train/entropy_loss</td><td>█▁</td></tr><tr><td>train/grad_norm</td><td>▁█</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/n_updates</td><td>▁█</td></tr><tr><td>train/policy_gradient_loss</td><td>█▁</td></tr><tr><td>train/value_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/length</td><td>3778.83333</td></tr><tr><td>eval/return</td><td>-20.23333</td></tr><tr><td>rollout/ep_len_mean</td><td>3512.44444</td></tr><tr><td>rollout/ep_rew_mean</td><td>-20.59259</td></tr><tr><td>time/fps</td><td>179</td></tr><tr><td>time/iterations</td><td>200</td></tr><tr><td>time/time_elapsed</td><td>89</td></tr><tr><td>time/total_episodes</td><td>27</td></tr><tr><td>time/total_timesteps</td><td>36000</td></tr><tr><td>train/entropy</td><td>1.79158</td></tr><tr><td>train/entropy_loss</td><td>-0.01792</td></tr><tr><td>train/grad_norm</td><td>0.06179</td></tr><tr><td>train/loss</td><td>-0.10954</td></tr><tr><td>train/n_updates</td><td>423</td></tr><tr><td>train/policy_gradient_loss</td><td>-0.10661</td></tr><tr><td>train/value_loss</td><td>0.01499</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test</strong> at: <a href='https://wandb.ai/ula_elfray/Homework_ActorCritic/runs/nij2ao0p' target=\"_blank\">https://wandb.ai/ula_elfray/Homework_ActorCritic/runs/nij2ao0p</a><br> View project at: <a href='https://wandb.ai/ula_elfray/Homework_ActorCritic' target=\"_blank\">https://wandb.ai/ula_elfray/Homework_ActorCritic</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250312_151428-nij2ao0p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_algorithm(\n",
    "    save_checkpoint_path='/content/drive/My Drive/rl_homework/a2c/test/2',\n",
    "    project='Homework_ActorCritic', run_name='test', seed=1,\n",
    "    eval_interval=9000, total_timesteps=40000,\n",
    "    resume_checkpoint_path='/content/drive/My Drive/rl_homework/a2c/test/1',\n",
    "    resume_run_id='nij2ao0p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sg16fJTe2S3k"
   },
   "source": [
    "Отдача за eval-эпизоды сохраняется в чекпоит в поле `evaluation_history`. При построении графиков для каждого значения `step` необходимо усреднить `returns` по результатам всех эпизодов из 2х запусков обучения. Кривые должны быть представлены в виде: среднее значение +/- [стандартная ошибка](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%B0%D0%BD%D0%B4%D0%B0%D1%80%D1%82%D0%BD%D0%B0%D1%8F_%D0%BE%D1%88%D0%B8%D0%B1%D0%BA%D0%B0) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uT-y6hdvaVVi",
    "outputId": "3debb686-9f41-4f70-932d-2f5c7410433a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-8280669dfc51>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load('/content/drive/My Drive/rl_homework/a2c/test/2/checkpoint.pt')['evaluation_history']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'step': 9008,\n",
       "  'returns': [-21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -20.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -20.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -20.0,\n",
       "   -20.0,\n",
       "   -19.0,\n",
       "   -20.0,\n",
       "   -20.0,\n",
       "   -20.0,\n",
       "   -19.0,\n",
       "   -19.0,\n",
       "   -20.0,\n",
       "   -19.0,\n",
       "   -18.0,\n",
       "   -19.0,\n",
       "   -21.0,\n",
       "   -18.0,\n",
       "   -16.0],\n",
       "  'lengths': [3056,\n",
       "   3056,\n",
       "   3168,\n",
       "   3171,\n",
       "   3280,\n",
       "   3304,\n",
       "   3350,\n",
       "   3371,\n",
       "   3408,\n",
       "   3480,\n",
       "   3544,\n",
       "   3612,\n",
       "   3614,\n",
       "   3648,\n",
       "   3656,\n",
       "   3700,\n",
       "   3721,\n",
       "   3746,\n",
       "   3915,\n",
       "   3919,\n",
       "   3970,\n",
       "   4067,\n",
       "   4098,\n",
       "   4154,\n",
       "   4168,\n",
       "   4179,\n",
       "   4230,\n",
       "   4372,\n",
       "   4866,\n",
       "   5293]},\n",
       " {'step': 18000,\n",
       "  'returns': [-21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -20.0,\n",
       "   -21.0,\n",
       "   -20.0,\n",
       "   -20.0,\n",
       "   -20.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -20.0,\n",
       "   -21.0,\n",
       "   -20.0,\n",
       "   -20.0,\n",
       "   -21.0,\n",
       "   -19.0,\n",
       "   -20.0,\n",
       "   -20.0,\n",
       "   -20.0,\n",
       "   -20.0,\n",
       "   -19.0,\n",
       "   -19.0,\n",
       "   -19.0,\n",
       "   -20.0,\n",
       "   -19.0],\n",
       "  'lengths': [3056,\n",
       "   3056,\n",
       "   3056,\n",
       "   3056,\n",
       "   3174,\n",
       "   3240,\n",
       "   3304,\n",
       "   3371,\n",
       "   3367,\n",
       "   3408,\n",
       "   3479,\n",
       "   3483,\n",
       "   3548,\n",
       "   3544,\n",
       "   3611,\n",
       "   3661,\n",
       "   3725,\n",
       "   3725,\n",
       "   3726,\n",
       "   3752,\n",
       "   3746,\n",
       "   3791,\n",
       "   3794,\n",
       "   3805,\n",
       "   3836,\n",
       "   3994,\n",
       "   4047,\n",
       "   4126,\n",
       "   4581,\n",
       "   4791]},\n",
       " {'step': 27008,\n",
       "  'returns': [-21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -20.0,\n",
       "   -20.0,\n",
       "   -20.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -20.0,\n",
       "   -21.0,\n",
       "   -20.0,\n",
       "   -20.0,\n",
       "   -20.0,\n",
       "   -21.0,\n",
       "   -19.0,\n",
       "   -20.0,\n",
       "   -20.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -20.0,\n",
       "   -19.0,\n",
       "   -20.0,\n",
       "   -20.0,\n",
       "   -19.0,\n",
       "   -19.0,\n",
       "   -19.0,\n",
       "   -19.0],\n",
       "  'lengths': [3056,\n",
       "   3056,\n",
       "   3168,\n",
       "   3280,\n",
       "   3296,\n",
       "   3350,\n",
       "   3479,\n",
       "   3514,\n",
       "   3544,\n",
       "   3595,\n",
       "   3590,\n",
       "   3648,\n",
       "   3721,\n",
       "   3706,\n",
       "   3727,\n",
       "   3739,\n",
       "   3749,\n",
       "   3798,\n",
       "   3804,\n",
       "   3843,\n",
       "   3860,\n",
       "   3896,\n",
       "   3962,\n",
       "   4100,\n",
       "   4107,\n",
       "   4163,\n",
       "   4174,\n",
       "   4254,\n",
       "   4280,\n",
       "   4304]},\n",
       " {'step': 36000,\n",
       "  'returns': [-21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -20.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -20.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -20.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -21.0,\n",
       "   -19.0,\n",
       "   -20.0,\n",
       "   -20.0,\n",
       "   -19.0,\n",
       "   -18.0,\n",
       "   -21.0,\n",
       "   -20.0,\n",
       "   -18.0,\n",
       "   -18.0,\n",
       "   -17.0],\n",
       "  'lengths': [3056,\n",
       "   3168,\n",
       "   3174,\n",
       "   3296,\n",
       "   3296,\n",
       "   3364,\n",
       "   3367,\n",
       "   3408,\n",
       "   3416,\n",
       "   3478,\n",
       "   3484,\n",
       "   3485,\n",
       "   3528,\n",
       "   3536,\n",
       "   3546,\n",
       "   3591,\n",
       "   3619,\n",
       "   3648,\n",
       "   3662,\n",
       "   3806,\n",
       "   3989,\n",
       "   4097,\n",
       "   4106,\n",
       "   4185,\n",
       "   4367,\n",
       "   4378,\n",
       "   4507,\n",
       "   4636,\n",
       "   5008,\n",
       "   5164]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('/content/drive/My Drive/rl_homework/a2c/test/2/checkpoint.pt')['evaluation_history']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhAnwt23aVVi"
   },
   "source": [
    "# Запуск обучения A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bi5Ujsk52S3k"
   },
   "outputs": [],
   "source": [
    "run_algorithm(\n",
    "    save_checkpoint_path='/content/drive/rl_homework/a2c/seed-0',\n",
    "    project='Homework_ActorCritic', run_name='a2c_seed-0', seed=0,\n",
    "    eval_interval=300000, total_timesteps=20000000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ul0rWqmm2S3k"
   },
   "outputs": [],
   "source": [
    "run_algorithm(\n",
    "    save_checkpoint_path='/content/drive/rl_homework/a2c/seed-100',\n",
    "    project='Homework_ActorCritic', run_name='a2c_seed-100', seed=100,\n",
    "    eval_interval=300000, total_timesteps=20000000,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hs-1UAZfaVVa",
    "jMyWS3dXdtQM",
    "AMNka5o-dxwE",
    "U6E9af0DeWt9",
    "83GHD_8i7QPz"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
